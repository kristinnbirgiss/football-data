{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db143e7-e2b8-4560-b6b4-6eb549a5659b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup the catalog and schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895aa45f-61f2-48fa-9d3a-aa282603f0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get or create Spark Session\n",
    "spark = SparkSession.builder.appName(\"SetupCatalogAndSchemas\").getOrCreate()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assuming you are using Unity Catalog, otherwise 'CATALOG_NAME' can be omitted.\n",
    "CATALOG_NAME = \"football_data_org\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "\n",
    "# 1. Set the Catalog Context (Optional, depending on your environment)\n",
    "# If you are NOT using Unity Catalog, skip this and the next step and assume 'default' catalog.\n",
    "print(f\"Using Catalog: {CATALOG_NAME}...\")\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "\n",
    "# 2. Create the Schemas (Databases) within the Catalog\n",
    "# Bronze: Raw, source-aligned data\n",
    "BRONZE_PATH = f\"{CATALOG_NAME}.{BRONZE_SCHEMA}\"\n",
    "print(f\"Creating Schema: {BRONZE_PATH} IF NOT EXISTS...\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_PATH}\")\n",
    "\n",
    "# Silver: Cleaned, validated, and conformed data\n",
    "SILVER_PATH = f\"{CATALOG_NAME}.{SILVER_SCHEMA}\"\n",
    "print(f\"Creating Schema: {SILVER_PATH} IF NOT EXISTS...\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SILVER_PATH}\")\n",
    "\n",
    "# Gold: Aggregated, business-ready data\n",
    "GOLD_PATH = f\"{CATALOG_NAME}.{GOLD_SCHEMA}\"\n",
    "print(f\"Creating Schema: {GOLD_PATH} IF NOT EXISTS...\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {GOLD_PATH}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Success! ETL Lakehouse structure is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b06efa7e-3984-461f-ba84-326bc6b942fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Bronze tables and ingest raw data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c60fcae-dd5d-431b-bd68-f04e075d9899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# API URLs\n",
    "MATCHES_API_URL = \"https://api.football-data.org/v4/competitions/PL/matches\"\n",
    "TEAMS_API_URL = \"https://api.football-data.org/v4/competitions/PL/teams\"\n",
    "PLAYERS_API_URL = \"https://api.football-data.org/v4/persons\"\n",
    "\n",
    "# Table paths\n",
    "MATCH_TABLE_PATH = f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.raw_matches\"\n",
    "TEAM_TABLE_PATH = f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.raw_teams\"\n",
    "PLAYER_TABLE_PATH = f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.raw_players\"\n",
    "\n",
    "# API credentials\n",
    "API_SCOPE = \"football_secrets\"\n",
    "API_KEY_NAME = \"football_api_key\"\n",
    "SEASON = 2025\n",
    "\n",
    "# Get API key from Databricks secrets (or use a dummy key locally)\n",
    "try:\n",
    "    api_key = dbutils.secrets.get(scope=API_SCOPE, key=API_KEY_NAME)\n",
    "except NameError:\n",
    "    print(\"WARNING: dbutils not available, using placeholder key\")\n",
    "    api_key = \"DUMMY_KEY\"\n",
    "\n",
    "headers = {\"X-Auth-Token\": api_key}\n",
    "params = {\"season\": SEASON}\n",
    "\n",
    "####################\n",
    "# Helper functions #\n",
    "####################\n",
    "\n",
    "# Fetch JSON data from the API\n",
    "def fetch_json(url, headers, params=None):\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "# Call the API for matches\n",
    "def fetch_matches(headers, params):\n",
    "    data = fetch_json(MATCHES_API_URL, headers, params)\n",
    "    matches = data.get(\"matches\", [])\n",
    "    print(f\"Fetched {len(matches)} matches\")\n",
    "    return matches\n",
    "\n",
    "# Call the API for teams\n",
    "def fetch_teams(headers, params):\n",
    "    data = fetch_json(TEAMS_API_URL, headers, params)\n",
    "    teams = data.get(\"teams\", [])\n",
    "    print(f\"Fetched {len(teams)} teams\")\n",
    "    return teams\n",
    "\n",
    "# Fetch a single person\n",
    "def fetch_person(person_id, headers):\n",
    "    url = f\"{PLAYERS_API_URL}/{person_id}\"\n",
    "    payload = fetch_json(url, headers)\n",
    "    return url, payload\n",
    "\n",
    "# Convert a list of items to a list of (id, json) tuples\n",
    "def to_records(items, id_key):\n",
    "    return [\n",
    "        (item.get(id_key), json.dumps(item))\n",
    "        for item in items\n",
    "    ]\n",
    "\n",
    "# Write the matches to the bronze table\n",
    "def write_matches_bronze(matches):\n",
    "    if not matches:\n",
    "        print(f\"No data for {MATCHES_API_URL}\")\n",
    "        return\n",
    "    match_records = to_records(matches, \"id\")\n",
    "\n",
    "    match_schema = [\"matchId\", \"payload_json\"]\n",
    "    df_matches = spark.createDataFrame(match_records, schema=match_schema)\n",
    "\n",
    "    match_bronze_df = (\n",
    "        df_matches\n",
    "        .withColumn(\"ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"source_api_url\", lit(MATCHES_API_URL))\n",
    "    )\n",
    "\n",
    "    match_bronze_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(MATCH_TABLE_PATH)\n",
    "\n",
    "    print(f\"Data written to {MATCH_TABLE_PATH}\")\n",
    "    return df_matches\n",
    "\n",
    "# Write the teams to the bronze table\n",
    "def write_teams_bronze(teams):\n",
    "    if not teams:\n",
    "        print(f\"No data for {TEAMS_API_URL}\")\n",
    "        return \n",
    "    team_records = to_records(teams, \"id\")\n",
    "\n",
    "    team_schema = [\"teamId\", \"payload_json\"]\n",
    "    df_teams = spark.createDataFrame(team_records, schema=team_schema)\n",
    "\n",
    "    team_bronze_df = (\n",
    "        df_teams\n",
    "        .withColumn(\"ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"source_api_url\", lit(TEAMS_API_URL))\n",
    "    )\n",
    "\n",
    "    team_bronze_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(TEAM_TABLE_PATH)\n",
    "\n",
    "    print(f\"Data written to {TEAM_TABLE_PATH}\")\n",
    "    return df_teams\n",
    "\n",
    "# Extract all person ids from the teams\n",
    "def extract_person_ids_from_teams(teams):\n",
    "    person_ids = set()\n",
    "    for team in teams:\n",
    "        squad = team.get(\"squad\", [])\n",
    "        for member in squad:\n",
    "            pid = member.get(\"id\")\n",
    "            if pid:\n",
    "                person_ids.add(pid)\n",
    "\n",
    "    person_ids = sorted(person_ids)\n",
    "    print(f\"Found {len(person_ids)} unique person ids in PL squads\")\n",
    "    return person_ids    \n",
    "\n",
    "# Fetch all persons from the API\n",
    "def fetch_all_persons(person_ids, headers, sleep_seconds=2):\n",
    "    person_records = []\n",
    "    for idx, pid in enumerate(person_ids, start=1):\n",
    "        url, payload = fetch_person(pid, headers)\n",
    "\n",
    "        person_records.append(\n",
    "            (\n",
    "                pid,\n",
    "                json.dumps(payload),\n",
    "                url,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"Fetched {idx}/{len(person_ids)} person_id={pid}\")\n",
    "        # Set to 2 because of the rate limit of 30 requests per minute\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    return person_records\n",
    "\n",
    "# Write the players to the bronze table\n",
    "def write_players_bronze(person_records):\n",
    "    if not person_records:\n",
    "        print(\"No person records to write\")\n",
    "        return\n",
    "\n",
    "    player_schema = [\"personId\", \"payload_json\", \"source_api_url\"]\n",
    "\n",
    "    df_players = spark.createDataFrame(person_records, schema=player_schema)\n",
    "    \n",
    "    # Add additional columns\n",
    "    player_bronze_df = (\n",
    "        df_players\n",
    "        .withColumn(\"ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"competition_code\", lit(\"PL\"))\n",
    "        .withColumn(\"source_system\", lit(\"football-data.org\"))\n",
    "    )\n",
    "    # Write to table\n",
    "    player_bronze_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(PLAYER_TABLE_PATH)\n",
    "\n",
    "    print(f\"Players written to {PLAYER_TABLE_PATH}\")\n",
    "\n",
    "def run_ingestion(headers, params):\n",
    "    matches = fetch_matches(headers=headers, params=params)\n",
    "    df_matches = write_matches_bronze(matches)\n",
    "\n",
    "    teams = fetch_teams(headers=headers, params=params)\n",
    "    df_teams = write_teams_bronze(teams)\n",
    "\n",
    "    person_ids = extract_person_ids_from_teams(teams)\n",
    "    person_records = fetch_all_persons(person_ids, headers=headers, sleep_seconds=2)\n",
    "    write_players_bronze(person_records)   \n",
    "\n",
    "try:\n",
    "    run_ingestion(headers=headers, params=params)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"API request failed: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937e3bb9-f7ee-4e9a-b3bd-1543996a3560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploring endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c2377e-5322-4325-bbe4-ff0bc184559a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- API Exploration Utility ---\n",
    "def explore_api_endpoint(endpoint_url, params=None, headers=None, max_keys=20):\n",
    "    \"\"\"\n",
    "    Fetches and displays a sample of the JSON response from the given API endpoint.\n",
    "    Does not ingest or persist any data.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(endpoint_url, headers=headers, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        print(f\"Endpoint: {endpoint_url}\")\n",
    "        print(f\"Top-level keys: {list(data.keys())[:max_keys]}\")\n",
    "        print(\"Sample data (truncated):\")\n",
    "        print(json.dumps(data, indent=2)[:5000])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or parse data from {endpoint_url}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "explore_api_endpoint(\"https://api.football-data.org/v4/competitions/PL/teams\", headers=headers)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
