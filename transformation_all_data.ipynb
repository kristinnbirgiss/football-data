{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7304fd1f-cd42-47d2-ac96-1a69a5c42182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d203068-1cf8-4fc2-89d6-0b661b06d2ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b25014-9299-4b9b-bf69-8fdd1f508c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, LongType\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, to_date, current_timestamp, when, lit\n",
    "\n",
    "# Define schema for payload_json\n",
    "match_schema = StructType([\n",
    "    StructField(\"area\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"code\", StringType())\n",
    "    ])),\n",
    "    StructField(\"competition\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"code\", StringType())\n",
    "    ])),\n",
    "    StructField(\"season\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"startDate\", StringType()),\n",
    "        StructField(\"endDate\", StringType())\n",
    "    ])),\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"utcDate\", StringType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"attendance\", IntegerType()),\n",
    "    StructField(\"venue\", StringType()),\n",
    "    StructField(\"matchday\", IntegerType()),\n",
    "    StructField(\"stage\", StringType()),\n",
    "    StructField(\"homeTeam\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType())\n",
    "    ])),\n",
    "    StructField(\"awayTeam\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType())\n",
    "    ])),\n",
    "    StructField(\"score\", StructType([\n",
    "        StructField(\"fullTime\", StructType([\n",
    "            StructField(\"home\", IntegerType()),\n",
    "            StructField(\"away\", IntegerType())\n",
    "        ])),\n",
    "        StructField(\"halfTime\", StructType([\n",
    "            StructField(\"home\", IntegerType()),\n",
    "            StructField(\"away\", IntegerType())\n",
    "        ]))\n",
    "    ])),\n",
    "    StructField(\"lastUpdated\", StringType())\n",
    "])\n",
    "\n",
    "# Parse JSON and rename columns\n",
    "CATALOG_NAME = \"football_data_org\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "\n",
    "df_bronze_matches = spark.table(f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.raw_matches\")\n",
    "\n",
    "df_matches_parsed = (\n",
    "    df_bronze_matches\n",
    "        .withColumn(\"data\", from_json(\"payload_json\", match_schema))\n",
    ")\n",
    "\n",
    "# To reorder columns so that an added column (e.g., \"kickoff_utc\") appears between two original columns,\n",
    "# use select() after all transformations to specify the desired order.\n",
    "\n",
    "df_matches_silver_base = (\n",
    "    df_matches_parsed\n",
    "        .select(\n",
    "            col(\"matchId\").alias(\"match_id\"),\n",
    "            col(\"data.area.code\").alias(\"area_code\"),\n",
    "            col(\"data.area.name\").alias(\"area_name\"),\n",
    "            col(\"data.competition.code\").alias(\"competition_code\"),            \n",
    "            col(\"data.status\").alias(\"status\"),\n",
    "            col(\"data.season.id\").alias(\"season_id\"),\n",
    "            col(\"data.stage\").alias(\"stage\"),\n",
    "            col(\"data.venue\").alias(\"venue\"),\n",
    "            col(\"data.attendance\").alias(\"attendance\"),\n",
    "            col(\"data.season.startDate\").alias(\"season_start_date_raw\"),\n",
    "            col(\"data.season.endDate\").alias(\"season_end_date_raw\"),\n",
    "            col(\"data.utcDate\").alias(\"utc_datetime_raw\"),\n",
    "            col(\"data.matchday\").alias(\"matchday\"),\n",
    "            col(\"data.homeTeam.id\").alias(\"home_team_id\"),\n",
    "            col(\"data.homeTeam.name\").alias(\"home_team_name\"),\n",
    "            col(\"data.awayTeam.id\").alias(\"away_team_id\"),\n",
    "            col(\"data.awayTeam.name\").alias(\"away_team_name\"),\n",
    "            col(\"data.score.fullTime.home\").alias(\"full_time_home_goals\"),\n",
    "            col(\"data.score.fullTime.away\").alias(\"full_time_away_goals\"),\n",
    "            col(\"data.score.halfTime.home\").alias(\"half_time_home_goals\"),\n",
    "            col(\"data.score.halfTime.away\").alias(\"half_time_away_goals\"),\n",
    "            col(\"ingest_ts\"),\n",
    "            col(\"source_api_url\")\n",
    "        )\n",
    "        .withColumn(\"kickoff_utc\", to_timestamp(\"utc_datetime_raw\"))\n",
    "        .withColumn(\"kickoff_time\", \n",
    "                    to_timestamp(\"utc_datetime_raw\").cast(\"timestamp\").substr(12,5))\n",
    "        .withColumn(\"match_date\", to_date(\"utc_datetime_raw\"))\n",
    "        .withColumn(\"season_start_date\", to_date(\"season_start_date_raw\"))\n",
    "        .withColumn(\"season_end_date\", to_date(\"season_end_date_raw\"))\n",
    "        .drop(\"utc_datetime_raw\", \"season_start_date_raw\", \"season_end_date_raw\")\n",
    "        .withColumn(\"silver_loaded_ts\", current_timestamp())\n",
    "        # Reorder columns: place \"kickoff_utc\" between \"competition_code\" and \"status\"\n",
    "        .select(\n",
    "            \"match_id\", \"status\", \"season_id\", \"stage\", \"area_code\", \"area_name\", \"competition_code\", \"venue\", \"attendance\",\n",
    "            \"matchday\", \"match_date\",\n",
    "            \"kickoff_time\",\"home_team_id\", \"home_team_name\", \"away_team_id\", \"away_team_name\",\n",
    "            \"full_time_home_goals\", \"full_time_away_goals\", \"half_time_home_goals\", \"half_time_away_goals\",\n",
    "            \"kickoff_utc\", \"season_start_date\", \"season_end_date\",\n",
    "            \"ingest_ts\", \"source_api_url\", \"silver_loaded_ts\"\n",
    "        )\n",
    ")\n",
    "\n",
    "\n",
    "# Handle nulls and invalid rows\n",
    "df_matches_flagged = (\n",
    "    df_matches_silver_base\n",
    "        .withColumn(\n",
    "            \"data_quality_issue\",\n",
    "            when(col(\"match_id\").isNull(), lit(\"MISSING_MATCH_ID\"))\n",
    "            .when(col(\"competition_code\").isNull(), lit(\"MISSING_COMPETITION\"))\n",
    "            .when(col(\"home_team_id\").isNull() | col(\"away_team_id\").isNull(), lit(\"MISSING_TEAM\"))\n",
    "            .when(col(\"kickoff_utc\").isNull(), lit(\"MISSING_KICKOFF\"))\n",
    "            .when(\n",
    "                (col(\"full_time_home_goals\") < 0) | (col(\"full_time_away_goals\") < 0),\n",
    "                lit(\"NEGATIVE_GOALS\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"is_valid_record\", col(\"data_quality_issue\").isNull())\n",
    ")\n",
    "\n",
    "df_matches_valid = df_matches_flagged.filter(col(\"is_valid_record\") == True)\n",
    "df_matches_invalid = df_matches_flagged.filter(col(\"is_valid_record\") == False)\n",
    "\n",
    "df_matches_valid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG_NAME}.{SILVER_SCHEMA}.matches_all_data\"\n",
    ")\n",
    "\n",
    "df_matches_invalid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG_NAME}.{SILVER_SCHEMA}.matches_all_data_quarantine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d158b60-1206-482a-9db4-13cfae312f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbc8e98-91c2-482b-b924-40ca95cf84a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, LongType\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, to_date, current_timestamp, when, lit\n",
    "\n",
    "\n",
    "# Schema for payload_json\n",
    "team_schema = StructType([\n",
    "    StructField(\"area\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"code\", StringType())\n",
    "    ])),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"shortName\", StringType()),\n",
    "    StructField(\"tla\", StringType()),\n",
    "    StructField(\"crest\", StringType()),\n",
    "    StructField(\"address\", StringType()),\n",
    "    StructField(\"website\", StringType()),\n",
    "    StructField(\"founded\", IntegerType()),\n",
    "    StructField(\"clubColors\", StringType()),\n",
    "    StructField(\"venue\", StringType()),\n",
    "    StructField(\"lastUpdated\", StringType())\n",
    "])\n",
    "\n",
    "# Parse and flatten\n",
    "df_bronze_teams = spark.table(f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.raw_teams\")\n",
    "\n",
    "df_teams_parsed = df_bronze_teams.withColumn(\n",
    "    \"data\", from_json(\"payload_json\", team_schema)\n",
    ")\n",
    "\n",
    "df_teams_silver_base = (\n",
    "    df_teams_parsed\n",
    "    .select(\n",
    "        col(\"teamId\").alias(\"team_id\"),\n",
    "        col(\"data.name\").alias(\"team_name\"),\n",
    "        col(\"data.shortName\").alias(\"short_name\"),\n",
    "        col(\"data.tla\").alias(\"tla\"),\n",
    "        col(\"data.area.id\").alias(\"area_id\"),\n",
    "        col(\"data.area.name\").alias(\"area_name\"),\n",
    "        col(\"data.area.code\").alias(\"area_code\"),\n",
    "        col(\"data.crest\").alias(\"crest_url\"),\n",
    "        col(\"data.founded\").alias(\"founded_year\"),\n",
    "        col(\"data.clubColors\").alias(\"club_colors\"),\n",
    "        col(\"data.venue\").alias(\"venue\"),\n",
    "        col(\"data.address\").alias(\"address\"),\n",
    "        col(\"data.website\").alias(\"website\"),\n",
    "        col(\"data.lastUpdated\").alias(\"last_updated_raw\"),\n",
    "        col(\"ingest_ts\"),\n",
    "        col(\"source_api_url\")\n",
    "    )\n",
    "    .withColumn(\"last_updated\", to_timestamp(\"last_updated_raw\"))\n",
    "    .drop(\"last_updated_raw\")\n",
    "    .withColumn(\"silver_loaded_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Quality rules\n",
    "df_teams_flagged = (\n",
    "    df_teams_silver_base\n",
    "    .withColumn(\n",
    "        \"data_quality_issue\",\n",
    "        when(col(\"team_id\").isNull(), lit(\"MISSING_TEAM_ID\"))\n",
    "        .when(col(\"team_name\").isNull(), lit(\"MISSING_TEAM_NAME\"))\n",
    "    )\n",
    "    .withColumn(\"is_valid_record\", col(\"data_quality_issue\").isNull())\n",
    ")\n",
    "\n",
    "df_teams_valid = df_teams_flagged.filter(col(\"is_valid_record\") == True)\n",
    "df_teams_invalid = df_teams_flagged.filter(col(\"is_valid_record\") == False)\n",
    "\n",
    "df_teams_valid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG_NAME}.{SILVER_SCHEMA}.teams_all_data\"\n",
    ")\n",
    "\n",
    "df_teams_invalid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG_NAME}.{SILVER_SCHEMA}.teams_all_data_quarantine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665f1223-e9ae-4816-91dc-1fe7c6b9491c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9c1c02-dd60-483c-b000-ddc1b75c3efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, LongType,  BooleanType\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, to_date, current_timestamp, when, lit, coalesce\n",
    "\n",
    "\n",
    "person_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"firstName\", StringType()),\n",
    "    StructField(\"lastName\", StringType()),\n",
    "    StructField(\"dateOfBirth\", StringType()),\n",
    "    StructField(\"nationality\", StringType()),\n",
    "    StructField(\"section\", StringType()),\n",
    "    StructField(\"position\", StringType()),\n",
    "    StructField(\"shirtNumber\", IntegerType()),\n",
    "    StructField(\"lastUpdated\", StringType()),\n",
    "    StructField(\"currentTeam\", StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"shortName\", StringType()),\n",
    "        StructField(\"tla\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "\n",
    "# Parse JSON and rename columns\n",
    "CATALOG_NAME = \"football_data_org\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "\n",
    "# Parse and flatten nested currentTeam\n",
    "df_bronze_persons = spark.table(f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.raw_players\")\n",
    "\n",
    "df_persons_parsed = df_bronze_persons.withColumn(\n",
    "    \"data\", from_json(\"payload_json\", person_schema)\n",
    ")\n",
    "\n",
    "df_persons_silver_base = (\n",
    "    df_persons_parsed\n",
    "    .select(\n",
    "        col(\"personId\").alias(\"person_id\"),\n",
    "        col(\"data.name\").alias(\"full_name\"),\n",
    "        col(\"data.firstName\").alias(\"first_name\"),\n",
    "        col(\"data.lastName\").alias(\"last_name\"),\n",
    "        col(\"data.dateOfBirth\").alias(\"date_of_birth_raw\"),\n",
    "        col(\"data.nationality\").alias(\"nationality\"),\n",
    "        col(\"data.section\").alias(\"section\"),\n",
    "        col(\"data.position\").alias(\"position\"),\n",
    "        col(\"data.shirtNumber\").alias(\"shirt_number\"),\n",
    "        col(\"data.lastUpdated\").alias(\"last_updated_raw\"),\n",
    "        col(\"data.currentTeam.id\").alias(\"current_team_id\"),\n",
    "        col(\"data.currentTeam.name\").alias(\"current_team_name\"),\n",
    "        col(\"data.currentTeam.tla\").alias(\"current_team_tla\"),\n",
    "        col(\"competition_code\"),\n",
    "        col(\"source_system\"),\n",
    "        col(\"ingest_ts\"),\n",
    "        col(\"source_api_url\")\n",
    "    )\n",
    "    .withColumn(\"date_of_birth\", to_date(\"date_of_birth_raw\"))\n",
    "    .withColumn(\"last_updated\", to_timestamp(\"last_updated_raw\"))\n",
    "    .drop(\"date_of_birth_raw\", \"last_updated_raw\")\n",
    "    .withColumn(\"silver_loaded_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Quality rules\n",
    "df_persons_flagged = (\n",
    "    df_persons_silver_base\n",
    "    .withColumn(\n",
    "        \"data_quality_issue\",\n",
    "        when(col(\"person_id\").isNull(), lit(\"MISSING_PERSON_ID\"))\n",
    "        .when(col(\"full_name\").isNull(), lit(\"MISSING_NAME\"))\n",
    "    )\n",
    "    .withColumn(\"is_valid_record\", col(\"data_quality_issue\").isNull())\n",
    ")\n",
    "\n",
    "df_persons_valid = df_persons_flagged.filter(col(\"is_valid_record\") == True)\n",
    "df_persons_invalid = df_persons_flagged.filter(col(\"is_valid_record\") == False)\n",
    "\n",
    "df_persons_valid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG_NAME}.{SILVER_SCHEMA}.players_all_data\"\n",
    ")\n",
    "\n",
    "df_persons_invalid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG_NAME}.{SILVER_SCHEMA}.players_all_data_quarantine\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transformation_all_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
